@inproceedings{denis:hal-03695835,
  TITLE = {{One core dedicated to MPI nonblocking communication progression? A model to assess whether it is worth it}},
  AUTHOR = {Denis, Alexandre and Jaeger, Julien and Jeannot, Emmanuel and Reynier, Florian},
  URL = {https://inria.hal.science/hal-03695835},
  BOOKTITLE = {{International Symposium on Cluster, Cloud and Internet Computing (CCGRID)}},
  ADDRESS = {Taormina, Italy},
  YEAR = {2022},
  MONTH = May,
  PDF = {https://inria.hal.science/hal-03695835/file/article.pdf},
  HAL_ID = {hal-03695835},
  HAL_VERSION = {v1},
}

@misc{internshipMathieu,
  author = {Mathieu Barbe},
  note   = {Chez Bull (Atos). Grenoble INP - ENSIMAG},
  title  = {Rapport de projet de fin d'études : Design and implement interrupt-handling code of a low-latency network adapter Linux driver},
  year   = {18 mars 2019 - 27 septembre 2019},
  url = {}
}

@inproceedings{denis:hal-01087775,
  TITLE = {{pioman: a pthread-based Multithreaded Communication Engine}},
  AUTHOR = {Denis, Alexandre},
  URL = {https://inria.hal.science/hal-01087775},
  BOOKTITLE = {{Euromicro International Conference on Parallel, Distributed and Network-based Processing}},
  ADDRESS = {Turku, Finland},
  YEAR = {2015},
  MONTH = Mar,
  KEYWORDS = {MPI ; pioman ; NewMadeleine},
  PDF = {https://inria.hal.science/hal-01087775/file/final.pdf},
  HAL_ID = {hal-01087775},
  HAL_VERSION = {v1},
}

@inproceedings{denis:hal-01324179,
  TITLE = {{MPI Overlap: Benchmark and Analysis}},
  AUTHOR = {Denis, Alexandre and Trahay, Fran{\c c}ois},
  URL = {https://inria.hal.science/hal-01324179},
  BOOKTITLE = {{International Conference on Parallel Processing}},
  ADDRESS = {Philadelphia, United States},
  SERIES = {45th International Conference on Parallel Processing },
  YEAR = {2016},
  MONTH = Aug,
  KEYWORDS = {MPI ;  overlap ;  benchmark},
  PDF = {https://inria.hal.science/hal-01324179/file/final.pdf},
  HAL_ID = {hal-01324179},
  HAL_VERSION = {v1},
}

@inproceedings{aumage:inria-00127356,
  TITLE = {{NewMadeleine: a Fast Communication Scheduling Engine for High Performance Networks}},
  AUTHOR = {Aumage, Olivier and Brunet, Elisabeth and Furmento, Nathalie and Namyst, Raymond},
  URL = {https://inria.hal.science/inria-00127356},
  BOOKTITLE = {{Workshop on Communication Architecture for Clusters (CAC 2007), workshop held in conjunction with IPDPS 2007}},
  ADDRESS = {Long Beach, California, United States},
  YEAR = {2007},
  MONTH = Mar,
  KEYWORDS = {NewMadeleine},
  PDF = {https://inria.hal.science/inria-00127356/file/cac-final.pdf},
  HAL_ID = {inria-00127356},
  HAL_VERSION = {v1},
}

@inproceedings{7312662,
  author    = {Derradji, Saïd and Palfer-Sollier, Thibaut and Panziera, Jean-Pierre and Poudes, Axel and Atos, François Wellenreiter},
  booktitle = {2015 IEEE 23rd Annual Symposium on High-Performance Interconnects},
  title     = {The BXI Interconnect Architecture},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {18-25},
  doi       = {10.1109/HOTI.2015.15}
}


@misc{portals4,
  title       = {Portals 4.0 Specification},
  note        = {\url{https://www.sandia.gov/portals/portals-4-0-specification-clone-2/}},
  url         = {https://www.sandia.gov/portals/portals-4-0-specification-clone-2/},
  lastvisited = {2023-08-01},
  key         = {portals4}
}

@misc{intelSoftwareDevMan,
  title       = {Intel® 64 and IA-32 Architectures, Software Developer's Manual},
  note        = {\url{https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html}},
  url         = {https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html},
  lastvisited = {2023-08-01},
  key         = {intelSoftwareDevMan}
}

@misc{intelUintrLinuxKernel,
  title       = {Intel Linux Kernel fork with Uintr},
  note        = {\url{https://github.com/intel/uintr-linux-kernel/tree/uintr-next}},
  url         = {https://github.com/intel/uintr-linux-kernel/tree/uintr-next},
  lastvisited = {2023-08-01},
  key         = {intelUintrLinuxKernel}
}

@misc{intelUintrLinuxKernelMan,
  title       = {Intel Uintr Linux man pages},
  note        = {\url{https://github.com/intel/uintr-linux-kernel/tree/uintr-next/tools/uintr/manpages}},
  url         = {https://github.com/intel/uintr-linux-kernel/tree/uintr-next/tools/uintr/manpages},
  lastvisited = {2023-08-01},
  key         = {intelUintrLinuxKernelMan}
}

@misc{linuxSignalMan,
  title       = {Signal Linux man pages},
  note        = {\url{https://man7.org/linux/man-pages/man2/sigaction.2.html}},
  url         = {https://man7.org/linux/man-pages/man2/sigaction.2.html},
  lastvisited = {2023-08-01},
  key         = {linuxSignalMan}
}

@misc{internshipSubject,
  title       = {Sujet de stage},
  note        = {\url{https://dept-info.labri.fr/~denis/Enseignement/Sujet_PFE_2023_uintr.html}},
  url         = {https://dept-info.labri.fr/~denis/Enseignement/Sujet_PFE_2023_uintr.html},
  lastvisited = {2023-08-01},
  key         = {internshipSubject}
}

@misc{pullRequestAltStackFix,
  title       = {Pull request for uintr_alt_stack() fix},
  note        = {\url{https://github.com/intel/uintr-linux-kernel/pull/2}},
  url         = {https://github.com/intel/uintr-linux-kernel/pull/2},
  lastvisited = {2023-08-01},
  key         = {pullRequestAltStackFix}
}

@misc{uintrLWN,
  title       = {Présentation du support des interruptions en espace utilisateur avec diapositive sur LWN.net},
  note        = {\url{https://lwn.net/Articles/869140/}},
  url         = {https://lwn.net/Articles/869140/},
  lastvisited = {2023-08-01},
  key         = {uintrLWN}
}

% mettre plutôt le papier d'origine ? ou le plus récent ?
@misc{hwloc,
  title       = {Portable Hardware Locality (hwloc)},
  note        = {\url{https://www.open-mpi.org/projects/hwloc/}},
  url         = {https://www.open-mpi.org/projects/hwloc/},
  lastvisited = {2023-08-01},
  key         = {hwloc}
}

@article{MICHAEL19981,
  title    = {Nonblocking Algorithms and Preemption-Safe Locking on Multiprogrammed Shared Memory Multiprocessors},
  journal  = {Journal of Parallel and Distributed Computing},
  volume   = {51},
  number   = {1},
  pages    = {1-26},
  year     = {1998},
  issn     = {0743-7315},
  doi      = {https://doi.org/10.1006/jpdc.1998.1446},
  url      = {https://www.sciencedirect.com/science/article/pii/S0743731598914460},
  author   = {Maged M. Michael and Michael L. Scott},
  keywords = {nonblocking, lock-free, mutual exclusion, locks, multiprogramming, concurrent queues, concurrent stacks, concurrent heaps, concurrent counters, concurrent date structures, , , .},
  abstract = {Most multiprocessors are multiprogrammed to achieve acceptable response time and to increase their utilization. Unfortunately, inopportune preemption may significantly degrade the performance of synchronized parallel applications. To address this problem, researchers have developed two principal strategies for a concurrent, atomic update of shared data structures: (1)preemption-safe lockingand (2)nonblocking(lock-free)algorithms. Preemption-safe locking requires kernel support. Nonblocking algorithms generally require a universal atomic primitive such ascompare-and-swaporload-linked/store-conditionaland are widely regarded as inefficient. We evaluate the performance of preemption-safe lock-based and nonblocking implementations of important data structures—queues, stacks, heaps, and counters—including nonblocking and lock-based queue algorithms of our own, in microbenchmarks and real applications on a 12-processor SGI Challenge multiprocessor. Our results indicate that our nonblocking queue consistently outperforms the best known alternatives and that data-structure-specific nonblocking algorithms, which exist for queues, stacks, and counters, can work extremely well. Not only do they outperform preemption-safe lock-based algorithms on multiprogrammed machines, they also outperform ordinary locks on dedicated machines. At the same time, since general-purpose nonblocking techniques do not yet appear to be practical, preemption-safe locks remain the preferred alternative for complex data structures: they outperform conventional locks by significant margins on multiprogrammed systems.}
}

% @article{10.1145/2038037.1941585,
%   author     = {Kogan, Alex and Petrank, Erez},
%   title      = {Wait-Free Queues with Multiple Enqueuers and Dequeuers},
%   year       = {2011},
%   issue_date = {August 2011},
%   publisher  = {Association for Computing Machinery},
%   address    = {New York, NY, USA},
%   volume     = {46},
%   number     = {8},
%   issn       = {0362-1340},
%   url        = {https://doi.org/10.1145/2038037.1941585},
%   doi        = {10.1145/2038037.1941585},
%   abstract   = {The queue data structure is fundamental and ubiquitous. Lock-free versions of the queue are well known. However, an important open question is whether practical wait-free queues exist. Until now, only versions with limited concurrency were proposed. In this paper we provide a design for a practical wait-free queue. Our construction is based on the highly efficient lock-free queue of Michael and Scott. To achieve wait-freedom, we employ a priority-based helping scheme in which faster threads help the slower peers to complete their pending operations. We have implemented our scheme on multicore machines and present performance measurements comparing our implementation with that of Michael and Scott in several system configurations.},
%   journal    = {SIGPLAN Not.},
%   month      = {feb},
%   pages      = {223–234},
%   numpages   = {12},
%   keywords   = {concurrent queues, wait-free algorithms}
% }

@inproceedings{10.1145/1941553.1941585,
  author    = {Kogan, Alex and Petrank, Erez},
  title     = {Wait-Free Queues with Multiple Enqueuers and Dequeuers},
  year      = {2011},
  isbn      = {9781450301190},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1941553.1941585},
  doi       = {10.1145/1941553.1941585},
  abstract  = {The queue data structure is fundamental and ubiquitous. Lock-free versions of the queue are well known. However, an important open question is whether practical wait-free queues exist. Until now, only versions with limited concurrency were proposed. In this paper we provide a design for a practical wait-free queue. Our construction is based on the highly efficient lock-free queue of Michael and Scott. To achieve wait-freedom, we employ a priority-based helping scheme in which faster threads help the slower peers to complete their pending operations. We have implemented our scheme on multicore machines and present performance measurements comparing our implementation with that of Michael and Scott in several system configurations.},
  booktitle = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming},
  pages     = {223–234},
  numpages  = {12},
  keywords  = {wait-free algorithms, concurrent queues},
  location  = {San Antonio, TX, USA},
  series    = {PPoPP '11}
}

@inproceedings{10.1145/1810479.1810540,
  author    = {Hendler, Danny and Incze, Itai and Shavit, Nir and Tzafrir, Moran},
  title     = {Flat Combining and the Synchronization-Parallelism Tradeoff},
  year      = {2010},
  isbn      = {9781450300797},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1810479.1810540},
  doi       = {10.1145/1810479.1810540},
  abstract  = {Traditional data structure designs, whether lock-based or lock-free, provide parallelism via fine grained synchronization among threads.We introduce a new synchronization paradigm based on coarse locking, which we call flat combining. The cost of synchronization in flat combining is so low, that having a single thread holding a lock perform the combined access requests of all others, delivers, up to a certain non-negligible concurrency level, better performance than the most effective parallel finely synchronized implementations. We use flat-combining to devise, among other structures, new linearizable stack, queue, and priority queue algorithms that greatly outperform all prior algorithms.},
  booktitle = {Proceedings of the Twenty-Second Annual ACM Symposium on Parallelism in Algorithms and Architectures},
  pages     = {355–364},
  numpages  = {10},
  keywords  = {concurrent data-structures, synchronization, multiprocessors},
  location  = {Thira, Santorini, Greece},
  series    = {SPAA '10}
}

% @inproceedings{10.1145/2145816.2145849,
%   author    = {Fatourou, Panagiota and Kallimanis, Nikolaos D.},
%   title     = {Revisiting the Combining Synchronization Technique},
%   year      = {2012},
%   isbn      = {9781450311601},
%   publisher = {Association for Computing Machinery},
%   address   = {New York, NY, USA},
%   url       = {https://doi.org/10.1145/2145816.2145849},
%   doi       = {10.1145/2145816.2145849},
%   abstract  = {Fine-grain thread synchronization has been proved, in several cases, to be outperformed by efficient implementations of the combining technique where a single thread, called the combiner, holding a coarse-grain lock, serves, in addition to its own synchronization request, active requests announced by other threads while they are waiting by performing some form of spinning. Efficient implementations of this technique significantly reduce the cost of synchronization, so in many cases they exhibit much better performance than the most efficient finely synchronized algorithms.In this paper, we revisit the combining technique with the goal to discover where its real performance power resides and whether or how ensuring some desired properties (e.g., fairness in serving requests) would impact performance. We do so by presenting two new implementations of this technique; the first (CC-Synch) addresses systems that support coherent caches, whereas the second (DSM-Synch) works better in cacheless NUMA machines. In comparison to previous such implementations, the new implementations (1) provide bounds on the number of remote memory references (RMRs) that they perform, (2) support a stronger notion of fairness, and (3) use simpler and less basic primitives than previous approaches. In all our experiments, the new implementations outperform by far all previous state-of-the-art combining-based and fine-grain synchronization algorithms. Our experimental analysis sheds light to the questions we aimed to answer.Several modern multi-core systems organize the cores into clusters and provide fast communication within the same cluster and much slower communication across clusters. We present an hierarchical version of CC-Synch, called H-Synch which exploits the hierarchical communication nature of such systems to achieve better performance. Experiments show that H-Synch significantly outper forms previous state-of-the-art hierarchical approaches.We provide new implementations of common shared data structures (like stacks and queues) based on CC-Synch, DSM-Synch and H-Synch. Our experiments show that these implementations outperform by far all previous (fine-grain or combined-based) implementations of shared stacks and queues.},
%   booktitle = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
%   pages     = {257–266},
%   numpages  = {10},
%   keywords  = {concurrent data structures, blocking algorithms, hierarchical algorithms, combining, synchronization techniques},
%   location  = {New Orleans, Louisiana, USA},
%   series    = {PPoPP '12}
% }

@article{10.1145/2370036.2145849,
  author     = {Fatourou, Panagiota and Kallimanis, Nikolaos D.},
  title      = {Revisiting the Combining Synchronization Technique},
  year       = {2012},
  issue_date = {August 2012},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {47},
  number     = {8},
  issn       = {0362-1340},
  url        = {https://doi.org/10.1145/2370036.2145849},
  doi        = {10.1145/2370036.2145849},
  abstract   = {Fine-grain thread synchronization has been proved, in several cases, to be outperformed by efficient implementations of the combining technique where a single thread, called the combiner, holding a coarse-grain lock, serves, in addition to its own synchronization request, active requests announced by other threads while they are waiting by performing some form of spinning. Efficient implementations of this technique significantly reduce the cost of synchronization, so in many cases they exhibit much better performance than the most efficient finely synchronized algorithms.In this paper, we revisit the combining technique with the goal to discover where its real performance power resides and whether or how ensuring some desired properties (e.g., fairness in serving requests) would impact performance. We do so by presenting two new implementations of this technique; the first (CC-Synch) addresses systems that support coherent caches, whereas the second (DSM-Synch) works better in cacheless NUMA machines. In comparison to previous such implementations, the new implementations (1) provide bounds on the number of remote memory references (RMRs) that they perform, (2) support a stronger notion of fairness, and (3) use simpler and less basic primitives than previous approaches. In all our experiments, the new implementations outperform by far all previous state-of-the-art combining-based and fine-grain synchronization algorithms. Our experimental analysis sheds light to the questions we aimed to answer.Several modern multi-core systems organize the cores into clusters and provide fast communication within the same cluster and much slower communication across clusters. We present an hierarchical version of CC-Synch, called H-Synch which exploits the hierarchical communication nature of such systems to achieve better performance. Experiments show that H-Synch significantly outper forms previous state-of-the-art hierarchical approaches.We provide new implementations of common shared data structures (like stacks and queues) based on CC-Synch, DSM-Synch and H-Synch. Our experiments show that these implementations outperform by far all previous (fine-grain or combined-based) implementations of shared stacks and queues.},
  journal    = {SIGPLAN Not.},
  month      = {feb},
  pages      = {257–266},
  numpages   = {10},
  keywords   = {combining, hierarchical algorithms, concurrent data structures, blocking algorithms, synchronization techniques}
}

% @inproceedings{10.1145/3018743.3019022,
%   author    = {Ramalhete, Pedro and Correia, Andreia},
%   title     = {POSTER: A Wait-Free Queue with Wait-Free Memory Reclamation},
%   year      = {2017},
%   isbn      = {9781450344937},
%   publisher = {Association for Computing Machinery},
%   address   = {New York, NY, USA},
%   url       = {https://doi.org/10.1145/3018743.3019022},
%   doi       = {10.1145/3018743.3019022},
%   abstract  = {Queues are a widely deployed data structure. They are used extensively in many multi threaded applications, or as a communication mechanism between threads or processes. We propose a new linearizable multi-producer-multi-consumer queue we named Turn queue, with wait-free progress bounded by the number of threads, and with wait-free bounded memory reclamation. Its main characteristics are: a simple algorithm that does no memory allocation apart from creating the node that is placed in the queue, a new wait-free consensus algorithm using only the atomic instruction compare-and-swap (CAS), and is easy to plugin with other algorithms for either enqueue or dequeue methods.},
%   booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
%   pages     = {453–454},
%   numpages  = {2},
%   keywords  = {non-blocking queue, low latency, wait-free},
%   location  = {Austin, Texas, USA},
%   series    = {PPoPP '17},
%   note       = {\url{https://github.com/pramalhe/ConcurrencyFreaks/blob/master/papers/crturnqueue-2016.pdf}}
% }

@article{10.1145/3155284.3019022,
  author     = {Ramalhete, Pedro and Correia, Andreia},
  title      = {POSTER: A Wait-Free Queue with Wait-Free Memory Reclamation},
  year       = {2017},
  issue_date = {August 2017},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {52},
  number     = {8},
  issn       = {0362-1340},
  url        = {https://doi.org/10.1145/3155284.3019022},
  doi        = {10.1145/3155284.3019022},
  abstract   = {Queues are a widely deployed data structure. They are used extensively in many multi threaded applications, or as a communication mechanism between threads or processes. We propose a new linearizable multi-producer-multi-consumer queue we named Turn queue, with wait-free progress bounded by the number of threads, and with wait-free bounded memory reclamation. Its main characteristics are: a simple algorithm that does no memory allocation apart from creating the node that is placed in the queue, a new wait-free consensus algorithm using only the atomic instruction compare-and-swap (CAS), and is easy to plugin with other algorithms for either enqueue or dequeue methods.},
  journal    = {SIGPLAN Not.},
  month      = {jan},
  pages      = {453–454},
  numpages   = {2},
  keywords   = {non-blocking queue, wait-free, low latency},
  note       = {\url{https://github.com/pramalhe/ConcurrencyFreaks/blob/master/papers/crturnqueue-2016.pdf}}
}

@inproceedings{10.1145/1989493.1989549,
  author  = {Fatourou, Panagiota and Kallimanis, Nikolaos},
  year    = {2011},
  month   = {06},
  pages   = {325-334},
  title   = {A highly-efficient wait-free universal construction},
  journal = {Annual ACM Symposium on Parallelism in Algorithms and Architectures},
  doi     = {10.1145/1989493.1989549}
}

@article{Fatourou2014,
  author   = {Fatourou, Panagiota
              and Kallimanis, Nikolaos D.},
  title    = {Highly-Efficient Wait-Free Synchronization},
  journal  = {Theory of Computing Systems},
  year     = {2014},
  month    = {Oct},
  day      = {01},
  volume   = {55},
  number   = {3},
  pages    = {475-520},
  abstract = {We study a simple technique, originally presented by Herlihy (ACM Trans. Program. Lang. Syst. 15(5):745--770, 1993), for executing concurrently, in a wait-free manner, blocks of code that have been programmed for sequential execution and require significant synchronization in order to be performed in parallel. We first present an implementation of this technique, called Sim, which employs a collect object. We describe a simple implementation of a collect object from a single shared object that supports atomic Add (or XOR) in addition to read; this implementation has step complexity O(1). By plugging in to Sim this implementation, Sim exhibits constant step complexity as well. This allows us to derive lower bounds on the step complexity of implementations of several shared objects, like Add, XOR, collect, and snapshot objects, from LL/SC objects.},
  issn     = {1433-0490},
  doi      = {10.1007/s00224-013-9491-y},
  url      = {https://doi.org/10.1007/s00224-013-9491-y}
}

% @inproceedings{10.1145/2851141.2851168,
%   author    = {Yang, Chaoran and Mellor-Crummey, John},
%   title     = {A Wait-Free Queue as Fast as Fetch-and-Add},
%   year      = {2016},
%   isbn      = {9781450340922},
%   publisher = {Association for Computing Machinery},
%   address   = {New York, NY, USA},
%   url       = {https://doi.org/10.1145/2851141.2851168},
%   doi       = {10.1145/2851141.2851168},
%   abstract  = {Concurrent data structures that have fast and predictable performance are of critical importance for harnessing the power of multicore processors, which are now ubiquitous. Although wait-free objects, whose operations complete in a bounded number of steps, were devised more than two decades ago, wait-free objects that can deliver scalable high performance are still rare.In this paper, we present the first wait-free FIFO queue based on fetch-and-add (FAA). While compare-and-swap (CAS) based non-blocking algorithms may perform poorly due to work wasted by CAS failures, algorithms that coordinate using FAA, which is guaranteed to succeed, can in principle perform better under high contention. Along with FAA, our queue uses a custom epoch-based scheme to reclaim memory; on x86 architectures, it requires no extra memory fences on our algorithm's typical execution path. An empirical study of our new FAA-based wait-free FIFO queue under high contention on four different architectures with many hardware threads shows that it outperforms prior queue designs that lack a wait-free progress guarantee. Surprisingly, at the highest level of contention, the throughput of our queue is often as high as that of a microbenchmark that only performs FAA. As a result, our fast wait-free queue implementation is useful in practice on most multi-core systems today. We believe that our design can serve as an example of how to construct other fast wait-free objects.},
%   booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
%   articleno = {16},
%   numpages  = {13},
%   keywords  = {non-blocking queue, wait-free, fast-path-slow-path},
%   location  = {Barcelona, Spain},
%   series    = {PPoPP '16}
% }

@article{10.1145/3016078.2851168,
  author     = {Yang, Chaoran and Mellor-Crummey, John},
  title      = {A Wait-Free Queue as Fast as Fetch-and-Add},
  year       = {2016},
  issue_date = {August 2016},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {51},
  number     = {8},
  issn       = {0362-1340},
  url        = {https://doi.org/10.1145/3016078.2851168},
  doi        = {10.1145/3016078.2851168},
  abstract   = {Concurrent data structures that have fast and predictable performance are of critical importance for harnessing the power of multicore processors, which are now ubiquitous. Although wait-free objects, whose operations complete in a bounded number of steps, were devised more than two decades ago, wait-free objects that can deliver scalable high performance are still rare.In this paper, we present the first wait-free FIFO queue based on fetch-and-add (FAA). While compare-and-swap (CAS) based non-blocking algorithms may perform poorly due to work wasted by CAS failures, algorithms that coordinate using FAA, which is guaranteed to succeed, can in principle perform better under high contention. Along with FAA, our queue uses a custom epoch-based scheme to reclaim memory; on x86 architectures, it requires no extra memory fences on our algorithm's typical execution path. An empirical study of our new FAA-based wait-free FIFO queue under high contention on four different architectures with many hardware threads shows that it outperforms prior queue designs that lack a wait-free progress guarantee. Surprisingly, at the highest level of contention, the throughput of our queue is often as high as that of a microbenchmark that only performs FAA. As a result, our fast wait-free queue implementation is useful in practice on most multi-core systems today. We believe that our design can serve as an example of how to construct other fast wait-free objects.},
  journal    = {SIGPLAN Not.},
  month      = {feb},
  articleno  = {16},
  numpages   = {13},
  keywords   = {fast-path-slow-path, non-blocking queue, wait-free}
}

@article{10.4230/lipics.disc.2019.28,
  doi       = {10.4230/LIPICS.DISC.2019.28},
  url       = {http://drops.dagstuhl.de/opus/volltexte/2019/11335/},
  author    = {Nikolaev, Ruslan},
  keywords  = {Computer Science, 000 Computer science, knowledge, general works},
  language  = {en},
  title     = {A Scalable, Portable, and Memory-Efficient Lock-Free FIFO Queue},
  publisher = {Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik GmbH, Wadern/Saarbruecken, Germany},
  year      = {2019},
  copyright = {Creative Commons Attribution 3.0 Unported license (CC-BY 3.0)}
}

% @inproceedings{10.1145/2442516.2442527,
%   author    = {Morrison, Adam and Afek, Yehuda},
%   title     = {Fast Concurrent Queues for X86 Processors},
%   year      = {2013},
%   isbn      = {9781450319225},
%   publisher = {Association for Computing Machinery},
%   address   = {New York, NY, USA},
%   url       = {https://doi.org/10.1145/2442516.2442527},
%   doi       = {10.1145/2442516.2442527},
%   abstract  = {Conventional wisdom in designing concurrent data structures is to use the most powerful synchronization primitive, namely compare-and-swap (CAS), and to avoid contended hot spots. In building concurrent FIFO queues, this reasoning has led researchers to propose combining-based concurrent queues.This paper takes a different approach, showing how to rely on fetch-and-add (F&amp;A), a less powerful primitive that is available on x86 processors, to construct a nonblocking (lock-free) linearizable concurrent FIFO queue which, despite the F&amp;A being a contended hot spot, outperforms combining-based implementations by 1.5x to 2.5x in all concurrency levels on an x86 server with four multicore processors, in both single-processor and multi-processor executions.},
%   booktitle = {Proceedings of the 18th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
%   pages     = {103–112},
%   numpages  = {10},
%   keywords  = {concurrent queue, fetch-and-add, nonblocking algorithm},
%   location  = {Shenzhen, China},
%   series    = {PPoPP '13}
% }

@article{10.1145/2517327.2442527,
  author     = {Morrison, Adam and Afek, Yehuda},
  title      = {Fast Concurrent Queues for X86 Processors},
  year       = {2013},
  issue_date = {August 2013},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {48},
  number     = {8},
  issn       = {0362-1340},
  url        = {https://doi.org/10.1145/2517327.2442527},
  doi        = {10.1145/2517327.2442527},
  abstract   = {Conventional wisdom in designing concurrent data structures is to use the most powerful synchronization primitive, namely compare-and-swap (CAS), and to avoid contended hot spots. In building concurrent FIFO queues, this reasoning has led researchers to propose combining-based concurrent queues.This paper takes a different approach, showing how to rely on fetch-and-add (F&amp;A), a less powerful primitive that is available on x86 processors, to construct a nonblocking (lock-free) linearizable concurrent FIFO queue which, despite the F&amp;A being a contended hot spot, outperforms combining-based implementations by 1.5x to 2.5x in all concurrency levels on an x86 server with four multicore processors, in both single-processor and multi-processor executions.},
  journal    = {SIGPLAN Not.},
  month      = {feb},
  pages      = {103–112},
  numpages   = {10},
  keywords   = {nonblocking algorithm, fetch-and-add, concurrent queue}
}

@inproceedings{10.1145/3490148.3538572,
  author    = {Nikolaev, Ruslan and Ravindran, Binoy},
  title     = {WCQ: A Fast Wait-Free Queue with Bounded Memory Usage},
  year      = {2022},
  isbn      = {9781450391467},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3490148.3538572},
  doi       = {10.1145/3490148.3538572},
  abstract  = {The concurrency literature presents a number of approaches for building non-blocking, FIFO, multiple-producer and multiple-consumer (MPMC) queues. However, only a fraction of them have high performance. In addition, many queue designs, such as LCRQ, trade memory usage for better performance. The recently proposed SCQ design achieves both memory efficiency as well as excellent performance. Unfortunately, both LCRQ and SCQ are only lock-free. On the other hand, existing wait-free queues are either not very performant or suffer from potentially unbounded memory usage. Strictly described, the latter queues, such as Yang \& Mellor-Crummey's (YMC) queue, forfeit wait-freedom as they are blocking when memory is exhausted. We present a wait-free queue, called wCQ. wCQ is based on SCQ and uses its own variation of fast-path-slow-path methodology to attain wait-freedom and bound memory usage. Our experimental studies on x86 and PowerPC architectures validate wCQ's great performance and memory efficiency. They also show that wCQ's performance is often on par with the best known concurrent queue designs.},
  booktitle = {Proceedings of the 34th ACM Symposium on Parallelism in Algorithms and Architectures},
  pages     = {307–319},
  numpages  = {13},
  keywords  = {ring buffer, fifo queue, wait-free},
  location  = {Philadelphia, PA, USA},
  series    = {SPAA '22}
}

@misc{wCQ,
  title       = {wCQ, SCQ, wCQ... Benchmark},
  note        = {\url{https://github.com/rusnikola/lfqueue}},
  url         = {https://github.com/rusnikola/lfqueue},
  lastvisited = {2023-08-06},
  key         = {wCQ}
}
