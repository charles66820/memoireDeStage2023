\section{Context}

\subsection{HPC}

Le stage c'est déroulé dans le context du Calcule Haute Performance (HPC, High-Performance Computing en anglais).
Le HPC utilise des supercalculateur pour la simulation numérique et le pré-apprentissage d'intelligences artificielles.
Ces simulation simule la dynamique des fluides, la résistance structurelle, les interaction moléculaire, les flux d'aire...

\bigskip

Elles couvrent différant domaines :
\begin{itemize}
  \item L'industrie : le médicales, l'automobiles, l'aviations, la constructions, l'aérospatiales...
  \item La défense : simulation atomique
  \item La recherche scientifiques : la création des galaxie, la fusion nucléaire, le climat...
  \item La météo
\end{itemize}

\bigskip

De nos jour les supercalculateurs sont composé de plusieurs ordinateur que l'on appelle noeud de calculs.
Ceci sont regroupé en grappe (cluster), il sont tous vue est utiliser comme une seul grande machine.
Ce fonctionnement pose des questions sur la répartition du calcule, la distribution de la mémoire, la communication entre les différant noeuds...
Dans le contexte du stage, nous nous sommes concentrés sur les communications entre noeuds.
Les noeuds sont connecté entre eux par un réseau haute performance.
Ce réseau est dédiés au communications est n'est pas forcément de type Ethernet.
La gestion des noeuds est généralement effectuée par un second réseau plus traditionnel (Ethernet; TCP/IP).
Ce genre de réseau ont une latence autour de quelque microseconde.
Chaque noeuds possède une ou plusieurs carte réseau et il faut les programmé.

\subsection{OS bypass}

Le stage ce passe donc aussi dans un context système.
Chaque noeuds à sont propre système d'exploitation qui permet la gestion des ressources, des processus, des fichiers, des périphériques.
Pour cela le système à 2 espaces :
\begin{itemize}
  \item un espace noyau où seul le code du système peut s'exécuter. Le code du système peut donc modifié n'importe quelle endrois de la mémoire, exécuté n'importe quelle instructions...
  \item un espace utilisateur où le code de l'utilisateur est exécuter. Cette espace est limité par le noyau qui controle ce que fait l'utilisateur.
\end{itemize}

En temps normale les périphériques sont programmé directement depuis le noyau du système d'exploitation ceci pour des questions de sécurité, de stendardisation des accés...
Lors que l'on passe par le noyau (kernel en anglais) on à un surcoût qui n'est pas négligeable dans notre cas.
Pour passer par le noyau on utilise généralement des appels système qui ce présente sous la forme d'une fonction.
Un appel système vas effectué un changement de contexte (context switch) pour passé de l'espace utilisateur à l'espace noyau.
Ce changement de context est coûteux car il sauvegarde les états du code de l'utilisateur avant d'exécuté celui du noyau.
Une fois le context switch effectué le code noyau de l'appel système s'exécute avant de refaire un context switch pour cette fois passé de l'espace noyau à l'espace utilisateur, et donc restoré l'états du code de l'utilisateur.
% Le kernel peut aussi exécuté aussi du code qu'il à mis à plus tard
Donc le fait de passé par un appel système coûte plusieurs microseconde.
On vois donc qu'on ne peut pas utiliser d'appel système car un appel système est déjà du même ordre de temps qu'une communication.
En HPC on programme donc directement la carte réseau à partir de l'espace utilisateur (OS bypass en anglais).
Pour cela on initialise toujours la carte à partir du noyau mais on fait une projection de la mémoire et des registre de la carte réseau dans l'espace d'adressage virtuelle du processus utilisateur.

Pour transmettre des événement à l'utilisateur les périphériques utilise généralement les interruptions, qui passe par le système donc on ne les utilise pas en HPC.
En HPC on fait donc du polling.

\subsection{Polling}

Le polling consiste à scruté (poll) régulièrement si on à reçus un événement.
Concrètement cela consiste à lire une zone mémoire modifier par la carte réseau et voire si un bit est passé à 1.
Pour cela il es possible de dédié un thread qui vas faire de l'attente active, scruté sans cesse si un événement à été reçus.
Mais on perd une unité de calcule lors ce que le thread est ordonnancer donc de la puissance de calcul, donc cette technique est peu utilisé. % ref
Il est aussi possible d'entrelacés le code de l'utilisateur avec des scrutation, c'est fréquemment utiliser mais ça oblige à l'utilisateur de prendre en compte la progression.
Une autre solution qui est utiliser par Pioman dans NewMadeleine (j'en parlerai plus tard) consiste à faire ces scrutation de manière opportuniste dans les threads qui on fini leur calcul, mais pour cela il faut déjà utilisé plusieurs thread et avoir une application qui à des calculs d'une durée hétérogène.

Il faut donc, peut importe la technique, régulièrement scruté pour faire progressé les communications.

\subsubsection{Inconvénients du polling}

%TODO: Définir réactivité au niveau MPI qui n'est pas le même au niveau ll.

Dans le cas d'un thread dédier qui fait de l'attente active la réactivité est excellente hormis quand il y à plus de thread que d'unité de calcule est que le thread n'est pas ordonnancer.

Quand on entrelaces le calcule est les scrutations la réactivité est moins bonne car il faut attendre que le calcule sois fini pour faire un poll.
C'est ce qui est fait habituellement.

Quand on utilise les thread de façon opportunist pour faire des scrutations la réactivité est moins bonne car il faut qu'il y est un thread disponible.

Donc un chois dois être fait entre perdre de la capacité de calcule ou perdre en réactivité.
En plus on perd un peut de temps de calcul à effectué du polling.

\subsection{BXI}

BXI est un type de réseau dédier pour le communications entre noeuds...

\begin{itemize}
  \item les carte réseaux BXI (Bull eXascale Interconnect) sont dévelopé par Atos
  \item elle sont capable de faire des communications sans intervention du CPU % offload
  \item le CPU à juste à soumettre une commande et la carte s'occupe du reste
  \item pour savoir si la communication est fini l'application peut poll ou recevoir une interruption en provenance de la carte
  \item portal4
  \item 
\end{itemize}

\subsection{MPI}

\begin{itemize}
  \item présenté MPI
  \item les communications asynchrone
  \item la progression ce fait :
  \begin{itemize}
    \item au moment des appelle a la biblio (MPI_Isend, MPI_Irecv, MPI_test, MPI_wait).
    \item grâce à un thread dédier
    \item grâce à une politique d'utilisation des threads de façon opportunist (quand ils ne font pas de calcule).
    \item ...
  \end{itemize}
  \item 
\end{itemize}

\subsection{NewMadeleine}

\begin{itemize}
  \item NewMadeleine est une bibliothèque de communication...
  \item elle est basé sur un système de progression asynchrone...
  \item supporte différant type de communication grâce a un système de driver (portal4 pour BXI, ibverb pour IB, shm...)
  \item ...
\end{itemize}

\subsection{Travaux antérieur}

\begin{itemize}
  \item Travaux de Mathieu Barbe durant un stage en 2019 sur l'utilisation d'interruption pour transmettre des événements réseau.
  \item Ces travaux vis à diminué la latence du au fait de passer par un driver noyaux.
  \item Il parle dans les perspective de directement traitement des interruptions depuis l'espace utilisateur ce qui mène à mon stage.
\end{itemize}
