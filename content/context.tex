\section{Contexte}

\subsection{HPC}

Le stage s'est déroulé dans le contexte du Calcul Haute Performance (HPC, High-Performance Computing en anglais).
Le HPC utilise des supercalculateurs pour la simulation numérique et le pré-apprentissage d'intelligences artificielles.
Ces simulations simulent la dynamique des fluides, la résistance structurelle, les interactions moléculaires, les flux d'air...

\bigskip

\noindent{}Elles couvrent différents domaines :
\begin{itemize}
  \item L'industrie : médical, automobile, aviation, construction, aérospatiale...
  \item La défense : simulation atomique...
  \item La recherche scientifique : création de galaxies, fusion nucléaire, climat...
  \item La météo
\end{itemize}

\bigskip

De nos jours, les supercalculateurs sont composés de plusieurs ordinateurs que l'on appelle "noeuds de calcul".
Ceux-ci sont regroupés en grappes (clusters) et sont tous vus et utilisés comme une seule grande machine.
Ce fonctionnement soulève des questions concernant la répartition du calcul, la distribution de la mémoire et la communication entre les différents noeuds.
Dans le contexte du stage, nous nous sommes concentrés sur les communications entre noeuds.
Les noeuds sont connectés entre eux par un réseau haute performance dédié aux communications, qui n'est pas forcément de type Ethernet.
La gestion des noeuds est généralement effectuée par un second réseau plus traditionnel (Ethernet ; TCP/IP).
Les réseaux haute performance ont une latence de l'ordre de quelques microsecondes ce qui n'est pas le cas des réseaux classiques qui ont une plus grande latence.
Avec ce type de réseaux, le débit est aussi bien plus élevé.
Chaque noeud possède une ou plusieurs cartes réseaux et il faut les programmer.

\subsection{OS bypass}

Le stage se passe donc aussi dans un contexte système.
Chaque noeud a son propre système d'exploitation qui permet la gestion des ressources, des processus, des fichiers, des périphériques.
Pour cela, le système deux espaces :
\begin{itemize}
  \item Un espace noyau (kernel en anglais) où seul le code du système peut s'exécuter.
  Le code du système peut donc modifier n'importe quel endroit de la mémoire, exécuter n'importe quelle instruction...
  \item Un espace utilisateur où le code de l'utilisateur est exécuté.
  Cet espace est limité par le noyau qui contrôle ce que fait l'utilisateur.
\end{itemize}

En temps normal, les périphériques sont programmés directement depuis le noyau du système d'exploitation, cela est fait pour des raisons de sécurité et de standardisation des accès.
Cependant, lorsque l'on passe par le noyau, il y a un surcoût qui n'est pas négligeable dans notre cas.
En effet, pour passer par le noyau, on utilise généralement des appels systèmes qui prennent la forme d'une fonction.
Un appel système effectue un changement de contexte (context switch) pour passer de l'espace utilisateur à l'espace noyau.
Ce changement de contexte est coûteux car il sauvegarde les états du code de l'utilisateur avant d'exécuter celui du noyau.
La sauvegarde de ces états consiste à sauvegarder les registres, les registres vectoriels, le pointeur d'instruction, le pointeur de pile et les flags d'états.
Ce qui rend coûteux le changement de contexte c'est aussi la \emph{MMU} pour "Memory Management Unit".
Cette unité permet la traduction des adresses virtuelles en adresses physiques et doit être mise à jour à chaque changement de contexte.
Cette \emph{MMU} utilise des tables de pages pour la traduction et ces tables sont propres à un processus ou au noyau.
L'adresse de cette table est mise à jour pendant le changement de contexte ce qui mène à l'invalidation des caches de la \emph{MMU}.
La traduction des adresses virtuelles est coûteuse car il faut la page donc faire des accès mémoire supplémentaire,
pour absorber ce coût la \emph{MMU} possède donc un cache nommé \emph{TLB}.
Ce \emph{TLB} garde les dernières traductions d'adresse, donc quand on fait un changement de contexte, il se retrouve changé par l'exécution du code du noyau ce qui fait que lors du changement de contexte du retour,
toutes les utilisations suivantes de la mémoire vont être coûteuses car il faut refaire les traductions d'adresse virtuelle en adresse physique pour recharger le \emph{TLB}.

% On change de table de page au près du MMU
% Le cache du MMU est invalidé

% Pour le possède un pointer vers la table des page du processus en cours.
% L'accès à une table de page est coûteux en accès mémoire (2 accès mémoires par page).
% Le TLB permet de mettre en cache les dernier pages manipuler pour ne pas payer c'est accès mémoire supplémentaire.
% Sans ce TLB tous les accès mémoire du code utilisateur coûteraient bien plus chère.
% Quand on fait un context switch le pointeur vers la table de pages change et le TLB ce retrouve invalidé car le noyau utilise aussi des pages et donc écrase la TLB.
% Donc le context switch et très coûteux pour le MMU qui fait la traduction grâce à la table des page et le TLB.

Une fois le changement de contexte effectué, le code noyau de l'appel système s'exécute avant de refaire un changement de contexte pour, cette fois, passer de l'espace noyau à l'espace utilisateur et ainsi restaurer l'état du code de l'utilisateur.
Le noyau peut aussi être amené à mettre à jour ses structures internes, exécuter des tâches qu'il avait mises à faire plus tard, etc.
Donc, le fait de passer par un appel système coûte plusieurs microsecondes.
On voit donc qu'il n'est pas préférable d'utiliser les appels système, car leur durée est du même ordre de grandeur qu'une communication entre noeuds.
En HPC, on programme donc directement la carte réseau à partir de l'espace utilisateur (OS bypass en anglais).
Pour cela, on initialise toujours la carte à partir du noyau, mais on fait une projection de la mémoire et des registres de la carte réseau dans l'espace d'adressage virtuel du processus utilisateur.

Pour transmettre des événements à l'utilisateur, les périphériques utilisent généralement les interruptions ordinaires, qui passent par le système donc on les utilise très peu en HPC.
En HPC on privilégie donc le polling que nous verrons en section \ref{sec:polling}.

\subsection{Les interruptions matérielles}

Les interruptions matérielles, également appelées IRQ pour "Interrupt ReQuest", existent depuis longtemps et
servent à remonter des événements extérieurs, par exemple d'un clavier, d'un port de communication, d'un port IDE, etc.
Elles ont ensuite été utilisées pour signaler des événements en provenance de périphériques plus variés, comme des cartes réseau.
Dans ce document, nous utiliserons le terme "interruption ordinaire" ou "IRQ" pour les désigner.

\subsection{Polling}
\label{sec:polling}

Il faut donc, peut importe la technique, régulièrement scruté pour faire progressé les communications.


Le \emph{polling} consiste à scruter (poll) régulièrement si un événement a été reçu.
Concrètement, cela consiste à lire une zone mémoire modifiée par la carte réseau et vérifier si un bit est passé à 1.
Pour cela, il est possible de dédier un thread qui effectuera une attente active, scrutant sans cesse si un événement a été reçu.
Cependant, cette technique entraîne la perte d'une unité de calcul quand le thread est ordonnancer, ce qui réduit la puissance de calcul, donc elle est utilisée dans certains cas comme expliqué dans cette article \cite{denis:hal-03695835}.
Une autre approche consiste à entrelacer le code de l'utilisateur avec les scrutations, ce qui est fréquemment utilisé, mais cela oblige l'utilisateur à prendre en compte la progression.
Une troisième solution, utilisée par \emph{Pioman} dans \emph{NewMadeleine} (nous en parlerons plus tard), consiste à effectuer ces scrutations de manière opportuniste dans les threads qui ont fini leur calcul, mais pour cela, il faut déjà utiliser plusieurs threads et avoir une application avec des calculs de durée hétérogène.

\subsubsection{Inconvénients du polling}

%TODO: Définir réactivité au niveau MPI qui n'est pas le même au niveau ll.

Dans le cas d'un thread dédié qui effectue une attente active, la réactivité est excellente, sauf lorsque le nombre de threads dépasse le nombre d'unités de calcul et lorsque le thread n'est pas ordonnancé.

Dans le cas où l'on entrelace le calcul et les scrutations, la réactivité est moins bonne car il faut attendre que le calcul soit terminé pour effectuer un poll.
C'est ce qui est habituellement fait.

Dans le cas où l'on utilise les threads de façon opportuniste pour effectuer des scrutations, la réactivité est moins bonne car il faut qu'il y ait un thread disponible.

Un choix doit donc être fait entre perdre de la capacité de calcul ou perdre en réactivité.
De plus, un peu de temps de calcul est perdu à effectuer du polling.

Dans le cas où l'on effectue trop de scrutations, on peut rencontrer des problèmes de contention mémoire.
En effet, le CPU manipule la mémoire par lignes de 8 mots mémoire appelés lignes de cache.
Ces lignes de cache font 512 bits sur un CPU 64 bits (car $8 * 64 = 512$), et 256 bits pour
des CPU 32 bits (car $8 * 32 = 256$).
Pour garantir la cohérence d'accès à la mémoire, ces lignes de cache sont invalidée lorsque quelqu'un d'autre les manipule.
Ainsi, si un thread scrute trop souvent une zone mémoire ou que plusieurs threads cherchent à scruter la même zone mémoire,
on aboutit à des problématiques de contention de la mémoire, car la ligne de cache est invalidée plus souvent, ce qui entraîne un surcoût.
Avec le polling nous somme donc contrains à respecter une granularité lié à la taille d'une ligne de cache.
Utiliser les interruptions permettrait de ne plus avoir ce problème, car seul le thread concerné par une zone mémoire serait prévenu donc plus de contention à ce niveau.

En plus, utiliser des interruptions permettrait de ne plus faire des scrutations pour rien,
mais seulement scruter lorsqu'il y a un événement à traiter.
Le nombre de scrutations serait donc grandement réduit.

\subsection{BXI}

\emph{BXI} \cite{7312662} pour "Bull eXascale Interconnect" est un type de réseau d'interconnexion (réseau haute performance) développé par \atos{}.
Historiquement développé par \emph{Bull} qui a été racheté par \atos{}, ce réseau est dédié aux communications entre noeuds.
Il est composé de cartes réseau \emph{BXI} et de switches \emph{BXI}.
Les cartes \emph{BXI} sont capables de faire progresser les communications réseau sans aucune intervention du CPU (offload des communications réseau).
Le CPU a juste à soumettre une commande dans une file sur la carte, et elle s'occupe de tout.
Le CPU peut ensuite récupérer une file d'événements pour savoir ce qu'il s'est passé, en somme faire un poll.
Les cartes sont également capables de déclencher des interruptions.
Les cartes implémente donc l'API de communication \emph{Portals4} \cite{portals4}.
Les utilisateurs utilisent donc cette API pour envoyer et recevoir des paquets réseau.

% Une des extensions \emph{Portals4} de \emph{BXI} permettent d'enregistrer un handler à une queue d'évènement (PtlEQAllocAsync())
% \url{https://bitbucketbdsfr.fsc.atos-services.net/projects/SHBRIL/repos/bxi-portals/browse/portals/PtlEQAllocAsync.3} TODO: pas accès

\subsection{MPI}

MPI, pour "Message Passing Interface", est un standard permettant de fournir une interface pour effectuer des communications entre plusieurs processus, souvent situés sur des noeuds différents, que l'on appelle \emph{processus MPI}.
Cette interface fournit une abstraction pour transmettre des données entre ces différents processus, masquant ainsi la complexité des communications.
L'interface permet d'envoyer et de recevoir des messages. Pour cela, il existe deux modes de communications :

\subsubsection{Les communications point à point}

C'est-à-dire entre deux processus MPI, également appelé One-to-One.
Pour ce faire, le processus MPI récepteur va appeler la fonction \code{MPI_Recv} qui est bloquante et va attendre la réception d'un message.
L'émetteur va lui faire un appel à la fonction \code{MPI_Send} qui est également bloquante et va envoyer un message, puis attendre que la communication soit terminée.
L'utilisation des fonctions \code{MPI_Send} / \code{MPI_Recv} bloque le code, ce qui nous fait perdre du temps à attendre.
Le standard MPI propose également une version non bloquante de ces fonctions, qui sont \code{MPI_Isend} et \code{MPI_Irecv}.
Cette version se contente de poster la communication et rend immédiatement la main.
Pour la progression et vérifier la terminaison, il faut donc utiliser d'autres fonctions comme \code{MPI_Test}, qui vérifie la progression et la fait si nécessaire, et la fonction \code{MPI_Wait}, qui attend activement la terminaison et s'occupe de la progression si nécessaire.
Il est important de noter que le standard MPI ne précise pas si la progression se fait en tâche de fond ou non, c'est aux implémentations de la norme MPI de choisir.
C'est pour cela que la progression peut se faire au niveau des fonctions \code{MPI_Wait} et \code{MPI_Test}, ou être faite avant, et donc l'appel aux fonctions s'occupe juste de la terminaison.
L'envoi des messages peut donc être asynchrone. % /* gardé cette phrase ? */

\subsubsection{Les communications collective}

Les communications collectives se font entre plusieurs processus.
Il en existe différents types :
\begin{itemize}
  \item Un processus vers plusieurs (One-to-All), par exemple un broadcast d'un message.
  \item De plusieurs processus vers un seul (All-to-One), par exemple une réduction (e.g. un processus reçoit la somme des valeurs des autres processus).
  \item De plusieurs processus vers plusieurs (All-to-All), par exemple lorsque tous les processus ont un message pour les autres.
\end{itemize}
Pour les communications collectives, il existe également deux versions, bloquante et non bloquante, qui fonctionnent de la même façon que les communications point à point.

\subsubsection{La progression en tâche de fond}

Pour faire progresser les communications en tâche de fond, il est possible de :

\begin{itemize}
  \item Faire régulièrement des appels à \code{MPI_test} et effectuer des calculs entre chaque appel.
  Cela permet de recouvrir la latence des communications par du calcul.
  La progression peut également se faire dans d'autres appels aux fonctions MPI.
  Lorsqu'il n'y a plus de calcul à effectuer, on repasse à une progression bloquante par un appel à \code{MPI_Wait},
  sauf si la communication est déjà terminée.
  \item Utiliser un thread dédié aux progressions.
  Dans ce cas, c'est la bibliothèque MPI ou l'application qui s'occupe de la progression des communications
  en tâche de fond grâce à un thread dédié.
  Il faut donc faire attention au placement des threads et prendre en compte qu'un thread est déjà utilisé pour les communications.
  Il faut aussi éviter d'appeler trop souvent \code{MPI_Test} car cela peut créer de la contention.
  \item Utilisation des threads de façon opportuniste, c'est-à-dire qu'un des threads, une fois son calcul terminé, fera progresser les communications.
  C'est ce qui est fait par \emph{Pioman} dans \emph{NewMadeleine}.
\end{itemize}

\subsection{NewMadeleine}

\emph{NewMadeleine} est une bibliothèque de communications qui prend en charge le RPC pour "Remote Procedure Call" et implémente également une interface MPI.
On peut donc la considérer comme une implémentation du standard MPI avec des fonctionnalités supplémentaires.
Elle est développée au Centre Inria de l'université de Bordeaux.
Elle est basée sur un système de progression événementielle, ce qui lui permet d'être asynchrone.
Elle est composée de modules, ce qui lui permet de charger dynamiquement des stratégies d'optimisation sur les paquets.
Ces stratégies sont l'agrégation de paquets, l'utilisation de priorités et le multi-rail. % /* TODO: précision multi-rail/split_balance */
Elle possède également un système de drivers pour supporter différents types de communications, tels que des réseaux
(e.g. portals4 pour BXI, ibverbs pour InfiniBand, psm2 pour OmniPath, ofi pour "Open Fabrics Interfaces", TCP) % Slingshot => cci ?
et des communications locales (shm pour "mémoire partagée", socket, self).

\subsection{Les signaux}
\label{sec:signal}

Les signaux\cite{linuxSignalMan} sont l'un des mécanismes permettant l'interface entre un processus et le système.
Pour cela, le processus utilisateur effectue des appels système pour définir les signaux pour lesquels il souhaite être notifié.
Pour être notifié, l'utilisateur peut définir un handler pour chaque signal auprès du système.
Ces handler sont déclenchés par le noyau qui prend en charge toutes les opérations nécessaires (sauvegarde de l'état du processus, changement de contexte, etc.).

Les signaux que l'utilisateur peut recevoir correspondent à des exceptions système (e.g. \emph{SIGFPE} pour une division par zéro ou \emph{SIGSEGV} pour une erreur de segmentation),
à des informations (e.g. \emph{SIGTERM} pour demander au processus de se fermer) ou à des notifications d'autres processus (i.e. \emph{SIGUSR1} et \emph{SIGUSR2}).
Il est possible de masquer la réception de certains signaux en utilisant des appels système de masquage.

\subsection{Travaux antérieurs}

Les origines de \emph{NewMadeleine} sont décrites dans cet article \cite{aumage:inria-00127356}.
Le système de progression dans \emph{NewMadeleine} fait l'objet de plusieurs travaux.
le fonctionnement du système de progression opportuniste \emph{Pioman} est décrit dans cet article\cite{denis:hal-01087775}.
Pour l'impact de l'overlap et la manière de le mesurer, l'article \cite{denis:hal-01324179} explique tout en détail.
Concernant l'utilisation des interruptions pour transmettre des événements réseau, les travaux de Mathieu Barbe pendant son stage en 2019 sont disponibles ici \cite{internshipMathieu}.
Ces travaux visent à réduire la latence due au passage par un driver noyau.
Il aborde également la perspective de traitement direct des interruptions depuis l'espace utilisateur, ce qui a conduit à ce stage.

Dans cette section il manque les travaux fait pour la progression des communications avec \emph{MVAPICH}, \emph{MPICH}, \emph{MxM}, etc.
%TODO: le faire pour tous ce qui existe
% Feedback d'Alexandre
% Pour la progression des communications, il y a des choses chez MVAPICH (avec du rdma), chez MPICH (avec un thread dédié, mais non-bindé), dans MxM pour InfiniBand (thread dédié)
% Pour éviter le polling et exploiter les interruptions, ça se faisait jusque là avec un appel système bloquant. Ça existe en particulier dans l'API verbs d'InfiniBand.

\subsection{Définitions}

Il est important d'avoir les définitions suivantes en tête :
\begin{itemize}
  \item \emph{CPU} : désigne la puce dans sa totalité.
  \item \emph{core} ou \emph{processeur} : désigne l'un des coeurs du \emph{CPU}.
  \item \emph{core logique}, \emph{processeur logique} ou \emph{unité de calcul} : % (pu pour Process Unit) ou threads materiel
  désigne une unité de calcul au sein d'un coeur. Il y en a deux par coeur dans les CPUs \intel{} Sapphire Rapids fourni par \atos{}.
\end{itemize}

Petit rappel : Un processus peut avoir plusieurs threads, un thread correspond à un fil d'exécution de code.
Et tous les threads au sein d'un même processus partage le même espace d'adressage virtuel.
