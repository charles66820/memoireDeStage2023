\section{Context}

\subsection{HPC}

Le stage c'est déroulé dans le context du Calcule Haute Performance (HPC, High-Performance Computing en anglais).
Le HPC utilise des supercalculateur pour la simulation numérique et le pré-apprentissage d'intelligences artificielles.
Ces simulation simule la dynamique des fluides, la résistance structurelle, les interaction moléculaire, les flux d'aire...

\bigskip

Elles couvrent différant domaines :
\begin{itemize}
  \item L'industrie : le médicales, l'automobiles, l'aviations, la constructions, l'aérospatiales...
  \item La défense : simulation atomique
  \item La recherche scientifiques : la création des galaxie, la fusion nucléaire, le climat...
  \item La météo
\end{itemize}

\bigskip

De nos jour les supercalculateurs sont composé de plusieurs ordinateur que l'on appelle noeud de calculs.
Ceci sont regroupé en grappe (cluster), il sont tous vue est utiliser comme une seul grande machine.
Ce fonctionnement pose des questions sur la répartition du calcule, la distribution de la mémoire, la communication entre les différant noeuds...
Dans le contexte du stage, nous nous sommes concentrés sur les communications entre noeuds.
Les noeuds sont connecté entre eux par un réseau haute performance.
Ce réseau est dédiés au communications est n'est pas forcément de type Ethernet.
La gestion des noeuds est généralement effectuée par un second réseau plus traditionnel (Ethernet; TCP/IP).
Les réseaux haute performance ont une latence autour de quelque microseconde.
Chaque noeuds possède une ou plusieurs carte réseau et il faut les programmé.

\subsection{OS bypass}

Le stage ce passe donc aussi dans un context système.
Chaque noeuds à sont propre système d'exploitation qui permet la gestion des ressources, des processus, des fichiers, des périphériques.
Pour cela le système à 2 espaces :
\begin{itemize}
  \item un espace noyau où seul le code du système peut s'exécuter. Le code du système peut donc modifié n'importe quelle endrois de la mémoire, exécuté n'importe quelle instructions...
  \item un espace utilisateur où le code de l'utilisateur est exécuter. Cette espace est limité par le noyau qui controle ce que fait l'utilisateur.
\end{itemize}

En temps normale les périphériques sont programmé directement depuis le noyau du système d'exploitation ceci pour des questions de sécurité, de stendardisation des accés...
Lors que l'on passe par le noyau (kernel en anglais) on à un surcoût qui n'est pas négligeable dans notre cas.
Pour passer par le noyau on utilise généralement des appels système qui ce présente sous la forme d'une fonction.
Un appel système vas effectué un changement de contexte (context switch) pour passé de l'espace utilisateur à l'espace noyau.
Ce changement de context est coûteux car il sauvegarde les états du code de l'utilisateur avant d'exécuté celui du noyau.
Une fois le context switch effectué le code noyau de l'appel système s'exécute avant de refaire un context switch pour cette fois passé de l'espace noyau à l'espace utilisateur, et donc restoré l'états du code de l'utilisateur.
% Le kernel peut aussi exécuté aussi du code qu'il à mis à plus tard
Donc le fait de passé par un appel système coûte plusieurs microseconde.
On vois donc qu'on ne peut pas utiliser d'appel système car un appel système est déjà du même ordre de temps qu'une communication.
En HPC on programme donc directement la carte réseau à partir de l'espace utilisateur (OS bypass en anglais).
Pour cela on initialise toujours la carte à partir du noyau mais on fait une projection de la mémoire et des registre de la carte réseau dans l'espace d'adressage virtuelle du processus utilisateur.

Pour transmettre des événement à l'utilisateur les périphériques utilise généralement les interruptions, qui passe par le système donc on ne les utilise pas en HPC.
En HPC on fait donc du polling.

\subsection{Polling}

Le polling consiste à scruté (poll) régulièrement si on à reçus un événement.
Concrètement cela consiste à lire une zone mémoire modifier par la carte réseau et voire si un bit est passé à 1.
Pour cela il es possible de dédié un thread qui vas faire de l'attente active, scruté sans cesse si un événement à été reçus.
Mais on perd une unité de calcule lors ce que le thread est ordonnancer donc de la puissance de calcul, donc cette technique est peu utilisé. % ref
Il est aussi possible d'entrelacés le code de l'utilisateur avec des scrutation, c'est fréquemment utiliser mais ça oblige à l'utilisateur de prendre en compte la progression.
Une autre solution qui est utiliser par Pioman dans NewMadeleine (j'en parlerai plus tard) consiste à faire ces scrutation de manière opportuniste dans les threads qui on fini leur calcul, mais pour cela il faut déjà utilisé plusieurs thread et avoir une application qui à des calculs d'une durée hétérogène.

Il faut donc, peut importe la technique, régulièrement scruté pour faire progressé les communications.

\subsubsection{Inconvénients du polling}

%TODO: Définir réactivité au niveau MPI qui n'est pas le même au niveau ll.

Dans le cas d'un thread dédier qui fait de l'attente active la réactivité est excellente hormis quand il y à plus de thread que d'unité de calcule est que le thread n'est pas ordonnancer.

Quand on entrelaces le calcule est les scrutations la réactivité est moins bonne car il faut attendre que le calcule sois fini pour faire un poll.
C'est ce qui est fait habituellement.

Quand on utilise les thread de façon opportunist pour faire des scrutations la réactivité est moins bonne car il faut qu'il y est un thread disponible.

Donc un chois dois être fait entre perdre de la capacité de calcule ou perdre en réactivité.
En plus on perd un peut de temps de calcul à effectué du polling.

\subsection{BXI}

BXI pour Bull eXascale Interconnect est un type de réseau d'interconnection (réseau haute performance) développé par Atos.
Historiquement développé par Bull qui à été racheté par Atos.
Ce réseau est dédier aux communications entre noeuds. Il est composé de carte réseau BXI et de switch BXI.
La carte BXI est capable de faire progressé le communication réseau sans aucune intervention du processeur (offload des communications réseau).
Le processeur à juste à soumettre une commande dans une file sur la carte et elle s'occupe de tous.
Le processeur peut ensuite récupéré une file d'événements pour savoir ce qu'il c'est passé, en somme faire un poll.
La carte est également capable de déclenché des interruptions.
L'utilisation de la carte passe donc par une implémentation du protocol Portal4.
L'utilisateur utilise donc le protocol pour envoyer et recevoir des paquets réseau.

\subsection{MPI}

MPI pour Message Passing Interface est une interface pour effectué des communications entre plusieurs processus, qui sont souvent sur des noeuds différant et qu'on appel \emph{processus MPI}.
Cette interface fourni une abstraction pour transmettre des données entre plusieurs processus.
L'abstraction masque la complexité des communications.
L'interface permet d'envoyer et de recevoir des messages. Pour cela il y a deux mode de communications :

\subsubsection{Les communications point à point}

C'est à dire entre deux processus MPI, aussi appelé One-to-One.
Pour ce faire le processus MPI récepteur vas appelé la fonction \code{MPI_Recv} qui est bloquante et vas attendre la reception d'un message.
L'émetteur vas lui faire un appel à la fonction \code{MPI_Send} qui est aussi bloquante et vas envoyer un message est attendre que la communication sois fini.
L'utilisation des fonctions \code{MPI_Send} / \code{MPI_Recv} est donc totalement synchrone ce qui bloque le code.
La nome propose aussi une version non bloquante de ces fonctions qui sont \code{MPI_Isend} et \code{MPI_Irecv}.
Cette version ce contente de posté la communication et rend immédiatement la main.
Pour la progression et verifier la terminaison il faut donc d'autre fonctions \code{MPI_Test} qui vérifie la progression, et la fait si nécessaire, et la fonction \code{MPI_Wait} qui attend activement la terminaison et s'occupe de la progression si nécessaire.
Il est important de noté que la norme ne précise pas si la progression ce fait en tâche de fond ou non, c'est aux implémentations de la nome MPI de choisir.
C'est pour cela que la progression peut ce faire au niveau de des fonctions \code{MPI_Wait} et \code{MPI_Test} ou être faire avant est donc l'appel aux fonctions s'occupe juste de la terminaison.
L'envoi des messages peut donc être asynchrone.

\subsubsection{Les communications collective}

Les communications collective ce fond entre plusieurs processus.
Il en existe de différant type :
\begin{itemize}
  \item un processus vers plusieurs (One-to-All) par example un broadcast d'un message
  \item de plusieurs processus vers un seul (All-to-One) par example une reduction (e.g. un processus reçois la somme des valeurs des autre processus)
  \item de plusieurs processus vers plusieurs (All-to-All) par example quand tous les processus on un message pour les autre
\end{itemize}
Pour les collective il existe également deux versions, bloquante et non bloquante, qui fonctionne de la même façon que les communications point à point.

\subsubsection{La progression asynchrone}

Pour faire progressé les communications de façon asynchrone il est possible de :

\begin{itemize}
  \item faire des appels à \code{MPI_test} régulièrement et faire du calcule entre chaque appel.
  Cela nous permet de recouvrir la latence des communications par du calcule.
  La progression ce faire également dans d'autre appel aux fonctions MPI.
  Quand il n'y à plus de calcul à faire on repasse à une progression synchrone par un appel à \code{MPI_Wait} sauf si la communication est déjà fini.
  \item utilisé un thread dédier aux progressions.
  Dans ce cas c'est la bibliothèque MPI qui s'occupe de la progression des communications en tâche de fond grâce à un thread dédié.
  Il faut donc faire attention au placement des threads et à prendre en compte qu'un thread est déjà utilisé par la bibliothèque de communication.
  Il faut aussi évité d'appelé trop souvent \code{MPI_Test} car ça crée de la contention
  \item utilisation des threads de façon opportunist c'est à dire quand un des thread à fini sont calcul il vas faire progressé les communications.
  C'est ce qui est faire par Pioman dans NewMadeleine.
\end{itemize}

\subsection{NewMadeleine}

\begin{itemize}
  \item NewMadeleine est une bibliothèque de communication...
  \item elle est basé sur un système de progression asynchrone...
  \item supporte différant type de communication grâce a un système de driver (portal4 pour BXI, ibverb pour IB, shm...)
  \item ...
\end{itemize}

\subsection{Travaux antérieur}

\begin{itemize}
  \item Travaux de Mathieu Barbe durant un stage en 2019 sur l'utilisation d'interruption pour transmettre des événements réseau.
  \item Ces travaux vis à diminué la latence du au fait de passer par un driver noyaux.
  \item Il parle dans les perspective de directement traitement des interruptions depuis l'espace utilisateur ce qui mène à mon stage.
\end{itemize}


% Cité ce qui à été fait pour la progression dan NewMadeleine...
% Le point de départ pour la progression avec pioman est ce papier : https://inria.hal.science/hal-01087775
% Sur l'overlap, celui-ci : https://inria.hal.science/hal-01324179v1
